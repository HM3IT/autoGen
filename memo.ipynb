{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMessageTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    " \n",
    "from autogen_agentchat.base import TaskResult\n",
    "from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.agents.web_surfer import MultimodalWebSurfer\n",
    "import os\n",
    "\n",
    "model = os.environ[\"MODEL_NAME\"]\n",
    "api_key = os.environ[\"API_KEY\"]\n",
    "base_url = os.environ[\"LLM_BASE_URL\"]\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=model,\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    "    parallel_tool_calls= False,\n",
    "    model_info={\n",
    "        \"family\": \"llama\",\n",
    "        \"vision\": False,\n",
    "        \"json_output\": True,\n",
    "        \"function_calling\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hm3 .chromadb_autogen\n",
      "---------- user ----------\n",
      "What is the weather in New York?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n",
      "source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n",
      "source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n",
      "source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n",
      "source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n",
      "source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n",
      "source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n",
      "DEBUG:autogen_agentchat.events:source='hm3_agent' models_usage=RequestUsage(prompt_tokens=190, completion_tokens=23) metadata={} content=[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')] type='ToolCallRequestEvent'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- hm3_agent ----------\n",
      "[FunctionCall(id='call_tvxbzyek', arguments='{\"city\":\"New York\",\"units\":\"\"}', name='get_weather')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n",
      "source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n",
      "source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n",
      "source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n",
      "source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n",
      "source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n",
      "source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n",
      "DEBUG:autogen_agentchat.events:source='hm3_agent' models_usage=None metadata={} content=[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)] type='ToolCallExecutionEvent'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- hm3_agent ----------\n",
      "[FunctionExecutionResult(content=\"Sorry, I don't know the weather in New York.\", name='get_weather', call_id='call_tvxbzyek', is_error=False)]\n",
      "---------- hm3_agent ----------\n",
      "Sorry, I don't know the weather in New York.\n",
      "---------- user ----------\n",
      "Where is the respsoen?\n",
      "---------- hm3_agent ----------\n",
      "The function to get the weather doesn't directly relate to getting a location. However, I can provide a generic function that handles locations if one exists in this system.\n",
      "\n",
      "Let me call another tool... \n",
      "\n",
      "Using \"get_location\"\n",
      "No direct response available for 'response', but here is similar output for the city of interest: New York.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "from autogen_agentchat import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME\n",
    "\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import MemoryContent, MemoryMimeType\n",
    "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.memory import ListMemory\n",
    "\n",
    "user_memory = ListMemory()\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# For trace logging.\n",
    "trace_logger = logging.getLogger(TRACE_LOGGER_NAME)\n",
    "trace_logger.addHandler(logging.StreamHandler())\n",
    "trace_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "print(str(Path.home()), \".chromadb_autogen\")\n",
    "# Initialize ChromaDB memory with custom config\n",
    "chroma_user_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"preferences\",\n",
    "        persistence_path=os.path.join(\"./.chromadb_autogen\"),\n",
    "        k=2,  # Return top  k results\n",
    "        score_threshold=0.4,  # Minimum similarity score\n",
    "    )\n",
    ")\n",
    "# a HttpChromaDBVectorMemoryConfig is also supported for connecting to a remote ChromaDB server\n",
    "\n",
    "# Add user preferences to memory\n",
    "# await chroma_user_memory.add(\n",
    "#     MemoryContent(\n",
    "#         content=\"The weather should be in metric units\",\n",
    "#         mime_type=MemoryMimeType.TEXT,\n",
    "#         metadata={\"category\": \"preferences\", \"type\": \"units\"},\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# await chroma_user_memory.add(\n",
    "#     MemoryContent(\n",
    "#         content=\"Meal recipe must be vegan\",\n",
    "#         mime_type=MemoryMimeType.TEXT,\n",
    "#         metadata={\"category\": \"preferences\", \"type\": \"dietary\"},\n",
    "#     )\n",
    "# )\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "\n",
    "async def get_weather(city: str, units: str = \"imperial\") -> str:\n",
    "    if units == \"imperial\":\n",
    "        return f\"The weather in {city} is 73 °F and Sunny.\"\n",
    "    elif units == \"metric\":\n",
    "        return f\"The weather in {city} is 23 °C and Sunny.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't know the weather in {city}.\"\n",
    "\n",
    " \n",
    "# Create assistant agent with ChromaDB memory\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"hm3_agent\",\n",
    "    model_client=model_client,\n",
    "    tools=[get_weather],\n",
    "    # memory=[user_memory],\n",
    "    # system_message=\"Use tools to solve user questions\"\n",
    ")\n",
    " \n",
    "await Console(assistant_agent.run_stream(task=\"What is the weather in New York?\"))\n",
    "await Console(assistant_agent.run_stream(task=\"Where is the respsoen?\"))\n",
    "\n",
    "await chroma_user_memory.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n",
      "---------- assistant_agent ----------\n",
      "[MemoryContent(content='The weather should be in metric units', mime_type='MemoryMimeType.TEXT', metadata={'category': 'preferences', 'mime_type': 'MemoryMimeType.TEXT', 'type': 'units', 'score': 0.4342842313626234, 'id': 'c20f0361-4d9e-4e22-bd4c-be8e8b5f98bd'}), MemoryContent(content='The weather should be in metric units', mime_type='MemoryMimeType.TEXT', metadata={'category': 'preferences', 'mime_type': 'MemoryMimeType.TEXT', 'type': 'units', 'score': 0.4342842313626234, 'id': 'd29eaab1-bff0-40ae-99a5-8585da6d3f5f'})]\n",
      "---------- assistant_agent ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
